{"ast":null,"code":"import './App.css';\nimport './funcs/processing.js';\nimport './funcs/processVars.js';\nimport './funcs/correlationVars.js';\nimport './funcs/correlation.js';\nimport './funcs/d3.min.js';\nimport './funcs/display';\nimport './funcs/displayVars';\nimport './funcs/docsUp';\nimport './funcs/docsUpVar';\nimport './funcs/downloads';\nimport './funcs/downloadsVars';\nimport './funcs/stoplist';\nimport './funcs/stoplistVars';\nimport './funcs/sweep';\nimport './funcs/sweepVars';\nimport './funcs/topicDocuments';\nimport './funcs/topictime';\nimport './funcs/topictimeVars';\nimport './funcs/topidDocVars';\nimport './funcs/vocab';\nimport './funcs/vocabVars';\nimport * as d3 from 'd3';\nd3.select(\"#sweep\").on(\"click\", function () {\n  requestedSweeps += 50;\n  timer = d3.timer(sweep);\n});\n\nfunction sweep() {\n  var startTime = Date.now();\n  var topicNormalizers = zeros(numTopics);\n\n  for (var topic = 0; topic < numTopics; topic++) {\n    topicNormalizers[topic] = 1.0 / (vocabularySize * topicWordSmoothing + tokensPerTopic[topic]);\n  }\n\n  for (var doc = 0; doc < documents.length; doc++) {\n    var currentDoc = documents[doc];\n    var docTopicCounts = currentDoc.topicCounts;\n\n    for (var position = 0; position < currentDoc.tokens.length; position++) {\n      var token = currentDoc.tokens[position];\n\n      if (token.isStopword) {\n        continue;\n      }\n\n      tokensPerTopic[token.topic]--;\n      var currentWordTopicCounts = wordTopicCounts[token.word];\n      currentWordTopicCounts[token.topic]--;\n\n      if (currentWordTopicCounts[token.topic] == 0) {//delete(currentWordTopicCounts[ token.topic ]);\n      }\n\n      docTopicCounts[token.topic]--;\n      topicNormalizers[token.topic] = 1.0 / (vocabularySize * topicWordSmoothing + tokensPerTopic[token.topic]);\n      var sum = 0.0;\n\n      for (var topic = 0; topic < numTopics; topic++) {\n        if (currentWordTopicCounts[topic]) {\n          topicWeights[topic] = (documentTopicSmoothing + docTopicCounts[topic]) * (topicWordSmoothing + currentWordTopicCounts[topic]) * topicNormalizers[topic];\n        } else {\n          topicWeights[topic] = (documentTopicSmoothing + docTopicCounts[topic]) * topicWordSmoothing * topicNormalizers[topic];\n        }\n\n        sum += topicWeights[topic];\n      } // Sample from an unnormalized discrete distribution\n\n\n      var sample = sum * Math.random();\n      var i = 0;\n      sample -= topicWeights[i];\n\n      while (sample > 0.0) {\n        i++;\n        sample -= topicWeights[i];\n      }\n\n      token.topic = i;\n      tokensPerTopic[token.topic]++;\n\n      if (!currentWordTopicCounts[token.topic]) {\n        currentWordTopicCounts[token.topic] = 1;\n      } else {\n        currentWordTopicCounts[token.topic] += 1;\n      }\n\n      docTopicCounts[token.topic]++;\n      topicNormalizers[token.topic] = 1.0 / (vocabularySize * topicWordSmoothing + tokensPerTopic[token.topic]);\n    }\n  } //console.log(\"sweep in \" + (Date.now() - startTime) + \" ms\");\n\n\n  completeSweeps += 1;\n  d3.select(\"#iters\").text(completeSweeps);\n\n  if (completeSweeps >= requestedSweeps) {\n    reorderDocuments();\n    sortTopicWords();\n    displayTopicWords();\n    plotMatrix();\n    vocabTable();\n    timeSeries();\n    timer.stop();\n  }\n}","map":{"version":3,"sources":["/Users/tatsukikuze12/Desktop/PastClasses/SummerResearch/jsLDAcopy/myapp/src/funcs/sweep.js"],"names":["d3","select","on","requestedSweeps","timer","sweep","startTime","Date","now","topicNormalizers","zeros","numTopics","topic","vocabularySize","topicWordSmoothing","tokensPerTopic","doc","documents","length","currentDoc","docTopicCounts","topicCounts","position","tokens","token","isStopword","currentWordTopicCounts","wordTopicCounts","word","sum","topicWeights","documentTopicSmoothing","sample","Math","random","i","completeSweeps","text","reorderDocuments","sortTopicWords","displayTopicWords","plotMatrix","vocabTable","timeSeries","stop"],"mappings":"AAAA,OAAO,WAAP;AACA,OAAO,uBAAP;AACA,OAAO,wBAAP;AACA,OAAO,4BAAP;AACA,OAAO,wBAAP;AACA,OAAO,mBAAP;AACA,OAAO,iBAAP;AACA,OAAO,qBAAP;AACA,OAAO,gBAAP;AACA,OAAO,mBAAP;AACA,OAAO,mBAAP;AACA,OAAO,uBAAP;AACA,OAAO,kBAAP;AACA,OAAO,sBAAP;AACA,OAAO,eAAP;AACA,OAAO,mBAAP;AACA,OAAO,wBAAP;AACA,OAAO,mBAAP;AACA,OAAO,uBAAP;AACA,OAAO,sBAAP;AACA,OAAO,eAAP;AACA,OAAO,mBAAP;AACA,OAAO,KAAKA,EAAZ,MAAoB,IAApB;AAIAA,EAAE,CAACC,MAAH,CAAU,QAAV,EAAoBC,EAApB,CAAuB,OAAvB,EAAgC,YAAW;AACvCC,EAAAA,eAAe,IAAI,EAAnB;AACAC,EAAAA,KAAK,GAAGJ,EAAE,CAACI,KAAH,CAASC,KAAT,CAAR;AACD,CAHH;;AAKA,SAASA,KAAT,GAAiB;AACb,MAAIC,SAAS,GAAGC,IAAI,CAACC,GAAL,EAAhB;AAEA,MAAIC,gBAAgB,GAAGC,KAAK,CAACC,SAAD,CAA5B;;AACA,OAAK,IAAIC,KAAK,GAAG,CAAjB,EAAoBA,KAAK,GAAGD,SAA5B,EAAuCC,KAAK,EAA5C,EAAgD;AAC9CH,IAAAA,gBAAgB,CAACG,KAAD,CAAhB,GAA0B,OAAOC,cAAc,GAAGC,kBAAjB,GAAsCC,cAAc,CAACH,KAAD,CAA3D,CAA1B;AACD;;AAED,OAAK,IAAII,GAAG,GAAG,CAAf,EAAkBA,GAAG,GAAGC,SAAS,CAACC,MAAlC,EAA0CF,GAAG,EAA7C,EAAiD;AAC/C,QAAIG,UAAU,GAAGF,SAAS,CAACD,GAAD,CAA1B;AACA,QAAII,cAAc,GAAGD,UAAU,CAACE,WAAhC;;AAEA,SAAK,IAAIC,QAAQ,GAAG,CAApB,EAAuBA,QAAQ,GAAGH,UAAU,CAACI,MAAX,CAAkBL,MAApD,EAA4DI,QAAQ,EAApE,EAAwE;AACtE,UAAIE,KAAK,GAAGL,UAAU,CAACI,MAAX,CAAkBD,QAAlB,CAAZ;;AACA,UAAIE,KAAK,CAACC,UAAV,EAAsB;AAAE;AAAW;;AAEnCV,MAAAA,cAAc,CAAES,KAAK,CAACZ,KAAR,CAAd;AACA,UAAIc,sBAAsB,GAAGC,eAAe,CAAEH,KAAK,CAACI,IAAR,CAA5C;AACAF,MAAAA,sBAAsB,CAAEF,KAAK,CAACZ,KAAR,CAAtB;;AACA,UAAIc,sBAAsB,CAAEF,KAAK,CAACZ,KAAR,CAAtB,IAAyC,CAA7C,EAAgD,CAC9C;AACD;;AACDQ,MAAAA,cAAc,CAAEI,KAAK,CAACZ,KAAR,CAAd;AACAH,MAAAA,gBAAgB,CAAEe,KAAK,CAACZ,KAAR,CAAhB,GAAkC,OAAOC,cAAc,GAAGC,kBAAjB,GAAsCC,cAAc,CAAES,KAAK,CAACZ,KAAR,CAA3D,CAAlC;AAEA,UAAIiB,GAAG,GAAG,GAAV;;AACA,WAAK,IAAIjB,KAAK,GAAG,CAAjB,EAAoBA,KAAK,GAAGD,SAA5B,EAAuCC,KAAK,EAA5C,EAAgD;AAC9C,YAAIc,sBAAsB,CAAEd,KAAF,CAA1B,EAAqC;AACnCkB,UAAAA,YAAY,CAAClB,KAAD,CAAZ,GACE,CAACmB,sBAAsB,GAAGX,cAAc,CAACR,KAAD,CAAxC,KACCE,kBAAkB,GAAGY,sBAAsB,CAAEd,KAAF,CAD5C,IAEFH,gBAAgB,CAACG,KAAD,CAHhB;AAID,SALD,MAMK;AACHkB,UAAAA,YAAY,CAAClB,KAAD,CAAZ,GACE,CAACmB,sBAAsB,GAAGX,cAAc,CAACR,KAAD,CAAxC,IACFE,kBADE,GAEFL,gBAAgB,CAACG,KAAD,CAHhB;AAID;;AACDiB,QAAAA,GAAG,IAAIC,YAAY,CAAClB,KAAD,CAAnB;AACD,OA5BqE,CA8BtE;;;AACA,UAAIoB,MAAM,GAAGH,GAAG,GAAGI,IAAI,CAACC,MAAL,EAAnB;AACE,UAAIC,CAAC,GAAG,CAAR;AACAH,MAAAA,MAAM,IAAIF,YAAY,CAACK,CAAD,CAAtB;;AACA,aAAOH,MAAM,GAAG,GAAhB,EAAqB;AACnBG,QAAAA,CAAC;AACDH,QAAAA,MAAM,IAAIF,YAAY,CAACK,CAAD,CAAtB;AACF;;AACFX,MAAAA,KAAK,CAACZ,KAAN,GAAcuB,CAAd;AAEApB,MAAAA,cAAc,CAAES,KAAK,CAACZ,KAAR,CAAd;;AACA,UAAI,CAAEc,sBAAsB,CAAEF,KAAK,CAACZ,KAAR,CAA5B,EAA6C;AAC3Cc,QAAAA,sBAAsB,CAAEF,KAAK,CAACZ,KAAR,CAAtB,GAAwC,CAAxC;AACD,OAFD,MAGK;AACHc,QAAAA,sBAAsB,CAAEF,KAAK,CAACZ,KAAR,CAAtB,IAAyC,CAAzC;AACD;;AACDQ,MAAAA,cAAc,CAAEI,KAAK,CAACZ,KAAR,CAAd;AAEAH,MAAAA,gBAAgB,CAAEe,KAAK,CAACZ,KAAR,CAAhB,GAAkC,OAAOC,cAAc,GAAGC,kBAAjB,GAAsCC,cAAc,CAAES,KAAK,CAACZ,KAAR,CAA3D,CAAlC;AACD;AACF,GA/DY,CAiEb;;;AACAwB,EAAAA,cAAc,IAAI,CAAlB;AACApC,EAAAA,EAAE,CAACC,MAAH,CAAU,QAAV,EAAoBoC,IAApB,CAAyBD,cAAzB;;AACA,MAAIA,cAAc,IAAIjC,eAAtB,EAAuC;AACrCmC,IAAAA,gBAAgB;AAChBC,IAAAA,cAAc;AACdC,IAAAA,iBAAiB;AACjBC,IAAAA,UAAU;AACVC,IAAAA,UAAU;AACVC,IAAAA,UAAU;AACVvC,IAAAA,KAAK,CAACwC,IAAN;AACD;AACF","sourcesContent":["import './App.css';\nimport './funcs/processing.js'\nimport './funcs/processVars.js'\nimport './funcs/correlationVars.js'\nimport './funcs/correlation.js'\nimport './funcs/d3.min.js'\nimport './funcs/display'\nimport './funcs/displayVars'\nimport './funcs/docsUp'\nimport './funcs/docsUpVar'\nimport './funcs/downloads'\nimport './funcs/downloadsVars'\nimport './funcs/stoplist'\nimport './funcs/stoplistVars'\nimport './funcs/sweep'\nimport './funcs/sweepVars'\nimport './funcs/topicDocuments'\nimport './funcs/topictime'\nimport './funcs/topictimeVars'\nimport './funcs/topidDocVars'\nimport './funcs/vocab'\nimport './funcs/vocabVars'\nimport * as d3 from 'd3';\n\n\n\nd3.select(\"#sweep\").on(\"click\", function() {\n    requestedSweeps += 50;\n    timer = d3.timer(sweep);\n  });\n\nfunction sweep() {\n    var startTime = Date.now();\n  \n    var topicNormalizers = zeros(numTopics);\n    for (var topic = 0; topic < numTopics; topic++) {\n      topicNormalizers[topic] = 1.0 / (vocabularySize * topicWordSmoothing + tokensPerTopic[topic]);\n    }\n  \n    for (var doc = 0; doc < documents.length; doc++) {\n      var currentDoc = documents[doc];\n      var docTopicCounts = currentDoc.topicCounts;\n  \n      for (var position = 0; position < currentDoc.tokens.length; position++) {\n        var token = currentDoc.tokens[position];\n        if (token.isStopword) { continue; }\n  \n        tokensPerTopic[ token.topic ]--;\n        var currentWordTopicCounts = wordTopicCounts[ token.word ];\n        currentWordTopicCounts[ token.topic ]--;\n        if (currentWordTopicCounts[ token.topic ] == 0) {\n          //delete(currentWordTopicCounts[ token.topic ]);\n        }\n        docTopicCounts[ token.topic ]--;\n        topicNormalizers[ token.topic ] = 1.0 / (vocabularySize * topicWordSmoothing + tokensPerTopic[ token.topic ]);\n  \n        var sum = 0.0;\n        for (var topic = 0; topic < numTopics; topic++) {\n          if (currentWordTopicCounts[ topic ]) {\n            topicWeights[topic] =\n              (documentTopicSmoothing + docTopicCounts[topic]) *\n              (topicWordSmoothing + currentWordTopicCounts[ topic ]) *\n            topicNormalizers[topic];\n          }\n          else {\n            topicWeights[topic] =\n              (documentTopicSmoothing + docTopicCounts[topic]) *\n            topicWordSmoothing *\n            topicNormalizers[topic];\n          }\n          sum += topicWeights[topic];\n        }\n  \n        // Sample from an unnormalized discrete distribution\n        var sample = sum * Math.random();\n          var i = 0;\n          sample -= topicWeights[i];\n          while (sample > 0.0) {\n            i++;\n            sample -= topicWeights[i];\n         }\n        token.topic = i;\n  \n        tokensPerTopic[ token.topic ]++;\n        if (! currentWordTopicCounts[ token.topic ]) {\n          currentWordTopicCounts[ token.topic ] = 1;\n        }\n        else {\n          currentWordTopicCounts[ token.topic ] += 1;\n        }\n        docTopicCounts[ token.topic ]++;\n  \n        topicNormalizers[ token.topic ] = 1.0 / (vocabularySize * topicWordSmoothing + tokensPerTopic[ token.topic ]);\n      }\n    }\n  \n    //console.log(\"sweep in \" + (Date.now() - startTime) + \" ms\");\n    completeSweeps += 1;\n    d3.select(\"#iters\").text(completeSweeps);\n    if (completeSweeps >= requestedSweeps) {\n      reorderDocuments();\n      sortTopicWords();\n      displayTopicWords();\n      plotMatrix();\n      vocabTable();\n      timeSeries();\n      timer.stop();\n    }\n  }"]},"metadata":{},"sourceType":"module"}